{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def getParameters(x):\n",
    "\n",
    "    #since x comes in as a batch with shape (20,4) -- (or any other batch size different from 20)\n",
    "\n",
    "    #let's condense X in one sample only, because we want only 4 elements, not 20*4 elements\n",
    "    xCondensed = K.mean(x,axis=0,keepdims=True)\n",
    "        #I'm using keepdims because we will need that x end up with the same number of samples for compatibility purposes (keras rules)\n",
    "\n",
    "    #let's expand x again (for compatibility purposes), now repeating the 4 values 20 (or more) times\n",
    "    #return K.ones_like(x) * xCondensed\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def yourFunction(x):\n",
    "\n",
    "\n",
    "    #now x has 4 parameters (assuming you had a Dense(4) before these lambda layers)\n",
    "    A1 = x[:,0]\n",
    "    b = x[:,1]\n",
    "    c = x[:,2]\n",
    "    d = x[:,3]\n",
    "\n",
    "    #creating the 20 (or more) iterations\n",
    "    ones = K.ones((20,1))\n",
    "    iterationsStartingAt1= K.cumsum(ones)\n",
    "    iterationsStartingAt0= iterationsStartingAt1 - 1\n",
    "    iterations = iterationsStartingAt0\n",
    "\n",
    "    return A1*np.exp(-(t+toff1)/t1)+A2*np.exp(-(t+toff2)/t2)+j0\n",
    "    return (a * K.sin((b*iterations) + c)) + d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cierpind\\Desktop\\Coding\\Machine_Learning\\keras_neural.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cierpind/Desktop/Coding/Machine_Learning/keras_neural.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cierpind/Desktop/Coding/Machine_Learning/keras_neural.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m#print(model.summary())\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/cierpind/Desktop/Coding/Machine_Learning/keras_neural.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m plot_model(model, to_file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel_plot.png\u001b[39m\u001b[39m'\u001b[39m, show_shapes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, show_layer_names\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\cierpind\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\vis_utils.py:445\u001b[0m, in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations, show_trainable)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Converts a Keras model to dot format and save to a file.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \n\u001b[0;32m    394\u001b[0m \u001b[39mExample:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39m  This enables in-line display of the model plots in notebooks.\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m--> 445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis model has not yet been built. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe model on a batch of data.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m     )\n\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_graphviz():\n\u001b[0;32m    452\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[0;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou must install pydot (`pip install pydot`) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mand install graphviz \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(see instructions at https://graphviz.gitlab.io/download/) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor plot_model to work.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "#commentary to this specific problem found here:\n",
    "#https://stackoverflow.com/questions/46227823/keras-neural-network-outputting-function-parameters-how-to-construct-loss-func\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "inputs_parameters = tf.keras.Input(shape=(300,))\n",
    "inputs_second_half = tf.keras.Input(shape=(200,))\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(Dense(units=input_size, activation='relu', input_dim=input_size))\n",
    "\n",
    "# Add hidden layers (you can customize the number of layers and units)\n",
    "num_hidden_layers = 2\n",
    "for _ in range(num_hidden_layers):\n",
    "    model.add(Dense(units=32, activation='relu'))\n",
    "\n",
    "# Add output layer\n",
    "model.add(Dense(units=output_size, activation='linear'))\n",
    "\n",
    "#model.add(Lambda(getParameters,output_shape=(4,),name='paramLayer'))\n",
    "#model.add(Lambda(yourFunction,output_shape=(1,),name='valueLayer'))\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "model.compile(optimizer='adam')\n",
    "\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function for the specified task.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth values, of shape (batch_size, 2, N).\n",
    "    - y_pred: Predicted values, of shape (batch_size, 7).\n",
    "\n",
    "    Returns:\n",
    "    - loss: Scalar value representing the mean loss over the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract x and y values from y_true\n",
    "    x_values = y_true[:, 0, :]  # Shape: (batch_size, N)\n",
    "    y_values = y_true[:, 1, :]  # Shape: (batch_size, N)\n",
    "    print(y_pred.shape)\n",
    "    \n",
    "\n",
    "    # Apply the function to each element of y_pred based on the corresponding x_value\n",
    "    # Result shape: (batch_size, N)\n",
    "    function_values = (\n",
    "        y_pred[:, 0, :] * K.exp(-(x_values + y_pred[:, 1, :]) / y_pred[:, 2, :]) +\n",
    "        y_pred[:, 3, :] * K.exp(-(x_values + y_pred[:, 4, :]) / y_pred[:, 5, :]) +\n",
    "        y_pred[:, 6, :]\n",
    "    )\n",
    "    #print(function_values)\n",
    "    # Compute the difference between the function values and y_values\n",
    "    # Result shape: (batch_size, N)\n",
    "    diff = function_values - y_values\n",
    "\n",
    "    # Take the square of the differences\n",
    "    squared_diff = K.square(diff)\n",
    "\n",
    "    # Take the mean over the batch\n",
    "    # Result shape: (batch_size,)\n",
    "    loss = K.mean(squared_diff, axis=-1)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def create_nn():\n",
    "    global X_data, points\n",
    "    # Input layer, the number of input nodes is governed by X_data.shape[1]\n",
    "    # X_data.shape[1] is the number of columns in X_data\n",
    "    inputs = keras.Input(shape=(X_data.shape[1],), name='input')\n",
    "\n",
    "    # Dense layers with relu activations\n",
    "    layers_dense = keras.layers.Dense(100, 'relu')(inputs)\n",
    "    # add a second dense layer with 50 hidden neurons and using the relu activation\n",
    "    layers_dense2 = keras.layers.Dense(50, 'relu')(layers_dense)\n",
    "    # Output layer, only one output node is used\n",
    "    parameters = keras.layers.Dense(7)(layers_dense2)\n",
    "    # Expand parameters to have same shape as y_true\n",
    "    expanded_parameters = keras.layers.RepeatVector(points)(parameters)\n",
    "\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=expanded_parameters, name=\"current_function_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 7)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (32,10) (32,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y_true \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(batch_size, \u001b[39m2\u001b[39m, lenght)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(batch_size, \u001b[39m7\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(custom_loss(y_true, y_pred))\n",
      "\u001b[1;32m/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(y_pred\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Apply the function to each element of y_pred based on the corresponding x_value\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Result shape: (batch_size, N)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m function_values \u001b[39m=\u001b[39m (\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     y_pred[:, \u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m K\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m(x_values \u001b[39m+\u001b[39m y_pred[:, \u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m y_pred[:, \u001b[39m2\u001b[39m]) \u001b[39m+\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     y_pred[:, \u001b[39m3\u001b[39m] \u001b[39m*\u001b[39m K\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m(x_values \u001b[39m+\u001b[39m y_pred[:, \u001b[39m4\u001b[39m]) \u001b[39m/\u001b[39m y_pred[:, \u001b[39m5\u001b[39m]) \u001b[39m+\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     y_pred[:, \u001b[39m6\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m#print(function_values)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Compute the difference between the function values and y_values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Result shape: (batch_size, N)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gyulaiemese/Desktop/Daniel/IRIS/Machine_Learning/keras_neural.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m diff \u001b[39m=\u001b[39m function_values \u001b[39m-\u001b[39m y_values\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32,10) (32,) "
     ]
    }
   ],
   "source": [
    "#test loss function\n",
    "import numpy as np\n",
    "batch_size = 32\n",
    "lenght = 10\n",
    "\n",
    "\n",
    "y_true = 255 * np.random.rand(batch_size, 2, lenght)\n",
    "y_pred = 255 * np.random.rand(batch_size, 7, lenght)\n",
    "\n",
    "print(custom_loss(y_true, y_pred))\n",
    "\n",
    "#print(K.eval(custom_loss(K.variable(y_true), K.variable(y_pred))).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
